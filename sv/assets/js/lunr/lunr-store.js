var store = [{
        "title": "Skapa en REST baserad sensor för Home Assistant med hjälp an pyscript integrationen",
        "excerpt":"Har du känt att att RESTful Sensorn i Home Assistant inte räcker till. Är REST API bakom nån form av inloggningsflöde du behöver gå igenom innan du kan anropa API’et, eller du kanske vill kombinera data från flera REST anrop till en sensor. Du kan alltid skapa din egen kustomiserade integratiom, men det är rätt omstädnigt.   Det finns en integration tillgänglig via HACS som heter pyscript som låter dig enkelt skapa sensorer och mycket mer.   För att köra igång pyscript i din Home Assistant installation genom att följa instruktionen nedan:      Först installera HACS om du inte redan gjort det.   Hitta pyscript integrationen i HACS och installera den.   Lägg till pyscript integration genom lägga till följande konfiguration     pyscript: allow_all_imports: true hass_is_global: true           SKapa en katalog som heter pyscript i rooten av din home assistant konfigurationskatalog     cd /path/to/home-assisant/config mkdir pyscript           Starta om Home Assistant för att aktivera integrationen.   Skapa en fil för din REST sensor i /pyscrupt/my-rest-sensor.py   Installera en riktig python kapabel editor, som Visual Studio Code (valfritt)   Det finns många sätt konfigurera pyscript, men med exmplet ovan så kommer pyscript hitta alla python filer i katalogen. Varje gång en fil ändras så kommer integration automatiskt ladda om filen. Håll ett öga på home-assistant.log filen efter fel medans du kodar.   Här är ett fullt exempel på en sensor som hämtar data från Svenska Krisinformations API Version 3 API. Sensorn är kompatibel med krisinfo-card kortet men använder senste versionen av API’et till skillnad från orginal integrationen.   Koden definierar en service metod för manuel uppdatering av sensor informationen samt en skedulerad uppdatering som pollar efter nytt data var 30e minut. Som man kan se så är det ganska enkelt skapa sensorer eller service metoder jämfört med skapa en fullständigt egen integration. Pyscript låter dig fokusera på logiken.   import requests from math import radians, sin, cos, acos  URL = \"https://api.krisinformation.se/v3/news?days={days}&amp;counties={counties}\" RADIUS = \"50\" SLAT = f'{hass.config.latitude}' SLON = f'{hass.config.longitude}' DAYS = 7 COUNTY_GOTHENBURG = 14  def getUrl(days, counties):     return URL.format(days=days, counties=counties)   def fetchData(url):     log.info(f\"Fetching data from: {url}\")     response = task.executor(requests.get, url)     if response.status_code != 200:         log.error(f\"Failure: {response.status_code}\")         return None      return response.json()  def make_object(attributes, index, element):     message = {}     message['Area'] = []      distance = None     within_range = False     is_in_county = False     is_in_country = False      for count, area in enumerate(element['Area']):         message['Area'].append({ \"Type\" : area['Type'], \"Description\" : area['Description'], \"Coordinate\" : area['Coordinate']})          if area['Type'] == 'Country':             is_in_country = True          if area['Type'] == 'County':             is_in_county = True          distance = calculate_distance(coords = area['Coordinate'])         if float(distance) &lt; float(RADIUS):             within_range = True      if within_range or is_in_county or is_in_country:         message['ID'] = element['Identifier']         message['Message'] = element['PushMessage']         message['Updated'] = element['Updated']         message['Published'] = element['Published']         message['Headline'] = element['Headline']         message['Preamble'] = element['Preamble']         message['BodyText'] = element['BodyText']         message['Web'] = element['Web']         message['Language'] = element['Language']         message['Event'] = element['Event']         message['SenderName'] = element['SenderName']         message['Links'] = []         if element['BodyLinks'] is not None:             for numbers, link in enumerate(element['BodyLinks']):                 message['Links'].append(link['Url'])         message['SourceID'] = element['SourceID']          attributes[\"messages\"].append(message)          if element['Event'] == \"Alert\":             attributes[\"alert_count\"] += 1         else:             attributes[\"news_count\"] += 1         attributes[\"total_count\"] += 1     else:         attributes[\"filtered_count\"] += 1   def calculate_distance(coords):     coords = coords.split()     coords = coords[0].split(',')     elon = coords[0]     elat = coords[1]      #Convert coordinates to radians     elat2 = radians(float(elat))     slat2 = radians(float(SLAT))     elon2 = radians(float(elon))     slon2 = radians(float(SLON))      #Calculate the distance between them     dist = 6371.01 * acos(sin(slat2)*sin(elat2) + cos(slat2)*cos(elat2)*cos(slon2 - elon2))      return dist   def parseData(json_data):     attributes = {}     attributes[\"messages\"] = []     attributes[\"news_count\"] = 0     attributes[\"alert_count\"] = 0     attributes[\"total_count\"] = 0     attributes[\"filtered_count\"] = 0     attributes[\"display_state\"] = \"No new messages\"     attributes[\"display_icon\"] = \"mdi:check-circle-outline\"     attributes[\"attribution\"] = \"krisinformation.se\"      for index, element in enumerate(json_data):         attributes[\"filtered_count\"] =+ 1         make_object(attributes = attributes, index = index, element = element)          if (attributes[\"news_count\"]&gt;0):             attributes[\"display_state\"] = f\"{attributes['news_count']} News Messages\"             attributes[\"display_icon\"] = \"mdi:alert-circle-outline\"          if (attributes[\"alert_count\"]&gt;0):             attributes[\"display_state\"] = f\"{attributes['alert_count']} Alert Messages\"             attributes[\"display_icon\"] = \"mdi:alert-circle\"      return attributes   def updateSensor(url, sensor_name, friendly_name):     log.debug(f\"Fetching data from: {url}\")     json_data = fetchData(url)     if json_data == None:         state.set(sensor_name, 0)         return      log.debug(f\"Got: {json_data}\")     current_state = 0     if sensor_name in state.names('sensor'):         current_state = state.get(sensor_name)     else:         state.set(sensor_name, 0)      attributes = parseData(json_data)     attributes['friendly_name'] = friendly_name      new_state = f\"{attributes['total_count']}\"      if current_state == new_state:         log.debug(f'State unchanged for: {sensor_name}')         return      log.info(f'Updating state: {sensor_name}')     state.set(sensor_name, new_state, attributes)   @time_trigger(\"cron(*/30 * * * *)\") def krisinformation_gbg():     url = getUrl(DAYS, COUNTY_GOTHENBURG)     updateSensor(url, 'sensor.krisinformation_goteborg', 'Krisinformation Göteborg')   @service def krisinformation_testing():     krisinformation_gbg()   Lycka till och lycklig pyscript hackande ! :smiley:  ","categories": ["home-assistant"],
        "tags": ["rest","home-assistant","pyscript","python"],
        "url": "/home-assistant/2023/06/19/home-assistant-creating-rest-sensors-using-pyscript-post.html",
        "teaser": null
      },{
        "title": "Sending Home Assistant logs to Elasticsearch (ELK)",
        "excerpt":"Do you want a better overview of you Home Assiatant logs? Do you want the home assistant logs to be searchable over time? Then you can export then to Elasticsearch and use Kibana for query the log data.           First you need to have an installed ELK stack (Elasticsearch, Logstash and Kibana). I went for an installation using docker with help of docker-elk github project.            Second you have to install Home Assiatant Logspout add-on.            Configure the logspout add-on       routes:    - multiline+logstash+tcp://mykibana.example.com:50000 env:    - name: SYSLOG_HOSTNAME      value: homeassistant    - name: INACTIVITY_TIMEOUT      value: 1m    - name: MULTILINE_PATTERN      value: &gt;-         (\\d\\d(\\d\\d)?[-/]\\d\\d[-/]\\d\\d[T         ]\\d\\d:\\d\\d:\\d\\d)|(^s6-rc:)|(^\\[\\d\\d:\\d\\d:\\d\\d\\])|(\\d\\d:\\d\\d:\\d\\d\\         -)|(^[TDIWEF]:)    - name: MULTILINE_MATCH      value: first    - name: INCLUDE_CONTAINERS      value: homeassistant    - name: LOGSTASH_FIELDS      value: source=my-home-assistant           Note that default all docker container logs in HAOS is sent to logstash for insertion into elasticsearch.            Configure logstash to parse the log data that comes in. My logstash.conf file looks like this:        input {    beats {       port =&gt; 5044    }     tcp {       port =&gt; 50000       codec =&gt; json    }     udp {       port  =&gt; 5000       codec =&gt; json    } }  ## Add your filters / logstash plugins configuration here filter {       if ([source] == \"my-home-assistant\") {          if ([docker][name] == \"/homeassistant\") {             grok {                patterns_dir =&gt; [\"/usr/share/logstash/pipeline/patterns\"]                match =&gt; { \"message\" =&gt; \"%{LOGLEVEL:log_level}%{SPACE}\\(%{GREEDYDATA:log_thread}\\)%{SPACE}\\[%{LOGGER_NAME:log_name}\\]%{SPACE}%{GREEDYDATA:log_message}\" }             }             mutate {       gsub =&gt; [ \"log_message\", \"\\x1B\\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|M|K]\", \"\" ]             }             if [log_message] =~ /\\n/ {                mutate {                   copy =&gt; { \"log_message\" =&gt; \"log_trace\" }                }                mutate {                   gsub =&gt; [ \"log_message\", \"(?m)^([^\\n]*)$.*\", \"\\1\" ]                }              }          } else {             drop { }          }       } }  output {    elasticsearch {       hosts =&gt; \"elasticsearch:9200\"       user =&gt; \"logstash_internal\"       password =&gt; \"${LOGSTASH_INTERNAL_PASSWORD}\"    } }           and my custom patterns file looks like this:       LOGGER_NAME [a-zA-Z0-9._-]+ UNICODE_START [\\\\u]           All incomming data tagged with my-home-assistant will be processed by the filter. It will also drop all data that comes from other than the HAOS home assistant docker container.   If you want to properly process data from other docker containers in the HAOS install, you will have to write more grok patterns.  ","categories": ["home-assistant"],
        "tags": ["home-assistant","ELK","elasticsearch","kibana","logstash"],
        "url": "/home-assistant/2023/06/19/sending-home-assistant-logs-to-elasticsearch-post.html",
        "teaser": null
      },{
        "title": "Using react-query for all kind of async logic",
        "excerpt":"Using @tanstack/react-query for REST calls performed by axios is a well known pattern. But you can use it for all kind of async logic. Here I will present how to use it with the browser speech synthesis API.   To make it easier to use the browser speech synthesis API I use the easy-speech library that abstracts away some complexity and browser diffrences.   First thing we need to do is to put the easy-speech speak method into use. Read the easy-speech documentation for how to init and use since this is out of scope for this blog post.   SpeechService.ts   import EasySpeech, { SpeechSynthesisVoice } from 'easy-speech';  export interface SpeakParams {    text: string;    voice: SpeechSynthesisVoice; }  const speak = ({ text, voice }: SpeakParams): Promise&lt;void&gt; =&gt; {    console.log('About to say:', text, ', using voice:', voice, 'of type', typeof voice);     return new Promise&lt;void&gt;((resolve, reject) =&gt; {       EasySpeech.speak({          text: text,          voice,          pitch: 1,          rate: 1,          volume: 1,          // there are more events, see the API for supported events          //boundary: (e: any) =&gt; console.debug('boundary reached', e)       })          .then(() =&gt; {             console.debug('Done saying:', text, ', using voice:', voice);             resolve();          })          .catch((error: any) =&gt; {             console.error('Failed saying:', text, ', using voice:', voice, error);             reject(error);          });    }); };  ...  const SpeechService = {    speak,    ... };  export default SpeechService;    Now lets connect this async speak method with react-query   useSpeak.ts   import { useMutation } from \"@tanstack/react-query\"; import SpeechService, { SpeakParams } from \"../services/SpeechService\";  interface UseSpeakParams {     onSuccess?: () =&gt; void;     onError?: (error: Error) =&gt; void; }  const performSpeak = async (params: SpeakParams) =&gt; {     try {         return await SpeechService.speak(params);     } catch (error: any) {         throw error;     } };  export const useSpeak = ({onSuccess = () =&gt; {},                           onError = () =&gt; {},                          }: UseSpeakParams) =&gt; {     const mutator = useMutation({mutationFn: performSpeak,         onSuccess: () =&gt; {             onSuccess();         },         onError: (error: Error) =&gt; {             onError(error);         },         retry: false,     });      return [         mutator.mutate,         { isSaving: mutator.isPending, ...mutator },     ] as const; };    And finally lets put the useSpeak hook into work in a react component.   SayHelloComponent.tsx   import React from \"react\"; import { useSpeak } from \"../hooks/useSpeak\"; import { SpeechSynthesisVoice } from \"easy-speech\"; import Select, { SingleValue } from \"react-select\"; import { useVoices } from \"../hooks/useVoices\"; import { SpeechSynthesisVoiceData } from \"../services/SpeechService\";  export const SayHelloComponent = () =&gt; {    const [text, setText] = React.useState&lt;string&gt;(\"Hi there! Are you ready?\");    const [voiceData, setVoiceData] = React.useState&lt;       SpeechSynthesisVoiceData | undefined    &gt;();     const [availableVoices, {isUseVoicesError, useVoicesError}] = useVoices();       const [speak] = useSpeak({       onError: (error: Error) =&gt; {          console.error(\"Failed speak:\", error);       },    });     React.useEffect(() =&gt; {       if (isUseVoicesError &amp;&amp; useVoicesError) {          console.log(\"Failed detect voices\", useVoicesError);       }    }, [isUseVoicesError, useVoicesError]);     const speech = () =&gt; {       if (voiceData) {          speak({ text, voice: voiceData.voice });       }    };     return (       &lt;div style=&gt;          &lt;Select             id=\"language\"             value={voiceData}             options={availableVoices as any}             onChange={(value: SingleValue&lt;SpeechSynthesisVoiceData&gt;) =&gt; {                if (value &amp;&amp; value.voice) {                   setVoiceData(value);                }             }}          /&gt;          &lt;input             type=\"text\"             value={text}             style=             onChange={(event) =&gt; {                setText(event?.target?.value || \"\");             }}          &gt;&lt;/input&gt;          &lt;button             style=             disabled={                !voiceData || !availableVoices || availableVoices.length === 0             }             onClick={() =&gt; {                speech();             }}          &gt;             Speak          &lt;/button&gt;       &lt;/div&gt;    ); };  export default SayHelloComponent;   Source code this blog post is based on could be found here.  ","categories": ["frontend"],
        "tags": ["typescript","frontend","react","react-query"],
        "url": "/frontend/2023/06/19/use-react-query-for-async-logic.html",
        "teaser": null
      },{
        "title": "Using webhooks to trigger actions when a docker image are pushed to a private docker registry",
        "excerpt":"Start by setting up your own docker registry. Here is a little shell script doing it for you   #/bin/sh  docker pull registry:2 docker stop registry docker rm registry  docker run -d \\   -p 5000:5000 \\   --restart=always \\   --name registry \\   -v ./registry:/var/lib/registry \\   -v ./config.yml:/etc/docker/registry/config.yml \\   registry:2  As you can see we are mounting a directory for the registry and a config file. Lets dig into the config file   version: 0.1 log:   fields:     service: registry storage:   cache:     blobdescriptor: inmemory   filesystem:     rootdirectory: /var/lib/registry http:   addr: :5000   headers:     X-Content-Type-Options: [nosniff] health:   storagedriver:     enabled: true     interval: 10s notifications:   endpoints:     - name: alistener       url: http://localhost:9000/hooks/redeploy-webhook       timeout: 500ms       threshold: 5       backoff: 1s  The secret to trigger something when you push to the registry is the notifications part in the end.   So lets continue with setup a simple webhook on your machine. For that we are using, webhook by Adnan Hajdarević.  There are several ways of install the software on your machine, I recommend docker or apt depending on your use case.   sudo apt-get install webhook   Now lets continue configure webhook software. Create a directory for it   mkdir /etc/webhook   Create a file named hooks.json and a directory named hooks and a file named git-redeploy.sh   touch /etc/webhook/hooks.json mkdir /etc/webhook/hooks touch /etc/webhook/hooks/git-redeploy.sh  Add the following content to the hooks.json file, don’t forget change the names and number of the docker images you  ant to perform some actions when a new image are pushed.   hooks.json  [   {     \"id\": \"redeploy-webhook\",     \"execute-command\": \"/etc/webhook/hooks/git-redeploy.sh\",     \"command-working-directory\": \"/etc/webhook/hooks\",     \"pass-arguments-to-command\": [       {         \"source\": \"payload\",         \"name\": \"events.0.target.repository\"       }     ],     \"trigger-rule\": {       \"and\": [         {           \"match\": {             \"type\": \"value\",             \"parameter\": {               \"source\": \"payload\",               \"name\": \"events.0.action\"             },             \"value\": \"push\"           }         },         {           \"or\": [             {               \"match\": {                 \"type\": \"value\",                 \"parameter\": {                   \"source\": \"payload\",                   \"name\": \"events.0.target.repository\"                 },                 \"value\": \"my-docker-image-1\"               }             },             {               \"match\": {                 \"type\": \"value\",                 \"parameter\": {                   \"source\": \"payload\",                   \"name\": \"events.0.target.repository\"                 },                 \"value\": \"my-docker-image-1\"               }             }           ]         },         {           \"match\": {             \"type\": \"value\",             \"parameter\": {               \"source\": \"payload\",               \"name\": \"events.0.target.tag\"             },             \"value\": \"latest\"           }         }       ]     }   } ]  git-redeploy.sh  #!/bin/sh  git_repository=$1   echo \"Redeploying $git_repository container...\" cd /home/admin/docker echo -n \"Current working directory: \" echo `pwd` echo \"\" if [[ \"$git_repository\" = \"my-docker-image-1\" ]]; then   echo \"Redeploying my-docker-service-1...\"   docker pull localhost:5000/my-docker-image-1:latest   docker stop my-docker-service-1   docker rm my-docker-service-1   docker run  --name my-docker-service-1 localhost:5000/my-docker-image-1:latest   echo \"Redeploying $git_repository container done!\" elif [[ \"$git_repository\" = \"my-docker-image-2\" ]]; then   echo \"Redeploying my-docker-service-2...\"   docker pull localhost:5000/my-docker-image-2:latest   docker stop my-docker-service-2   docker rm my-docker-service-2   docker run  --name my-docker-service-2 localhost:5000/my-docker-image-2:latest   echo \"Redeploying $git_repository container done!\" else   echo \"\"                                              echo \"ERROR: Redeploy of unknown repository  $git_repository failed !!!\"   exit -1 fi  exit 0  The webhook software could be executed as a service, how to achieve that depends heavily on your OS. This is an example for alpine linux.   /etc/init.d/webhook  #!/sbin/openrc-run  name=\"webhook\" command=\"/usr/bin/webhook\" command_args=\"-hooks /etc/webhook/hooks.json -verbose -logfile=/var/log/webhook.log tunnel run &amp;\" pidfile=\"/var/run/webhook.pid\"  depend() {         need net localmount         after firewall }  Enable startup of service at boot time and then start it up   # Start service ay boot time rc-update add webhook default  # Start service rc-service webhook start  ","categories": ["docker","ci/cd"],
        "tags": ["docker","docker-registry","webhooks","ci/cd"],
        "url": "/docker/ci/cd/2023/10/09/trigger-action-when-continer-pushed.html",
        "teaser": null
      },{
        "title": "How to build docker containers using gitea runners",
        "excerpt":"Gitea has pretty recently introduced actions, which is basically CI/CD pipelines.  They work very match as Gitlab Actions.  In fact a Gitea CI/CD pipeline is almost fully compatible with a Gitlab pipeline except a few minor details and limitations. You can even use Gitlab actions in your Gitea CI/CD pipeline build steps.   This article is about how to make your Gitea CI/CD pipeline build docker images.   Start by setting up gitea, installation with docker. Don’t forget to enable actions in Gitea configuration file.   [actions] ENABLED=true  I also suggest you setup HTTPS with a valid certificate, you will run into problems else in your builds with  complains about insecure stuff. How to do that is out of scope for this blog post, but you can read more about it here.   After that it’s time to set up the runners.  The documentation do not say anything about how to build different kind of artifacts. What you can build with a runner is defined by it’s LABELS. A label maps to a docker container performing the build step in the CI/CD pipeline. If you not specify any LABELS you get the default that can NOT build docker containers. To get that capability you can make use of the catthehacker docker images Github project.   Anyway this is how I setup my runners      Create directory for your runners     mkdir gitea-runners           Then create a initial config file for the runner, binary could be downloaded here.     ./act_runner generate-config &gt; config1.yaml           Now lets create a basic script file for fire up you runner docker container.  How to get the token is covered by the Gitea Action Quick start guide.     #!/bin/sh  docker pull gitea/act_runner:latest docker stop nazgul-runner-1 docker rm nazgul-runner-1 docker run \\ --restart always \\ -v $PWD/config1.yaml:/config.yaml \\ -v $PWD/data1:/data \\ -v /var/run/docker.sock:/var/run/docker.sock:rw \\ -e CONFIG_FILE=/config.yaml \\ -e GITEA_INSTANCE_URL=https://&lt;gitea.mydomain.con&gt;/ \\ -e GITEA_RUNNER_REGISTRATION_TOKEN=&lt;MyGiteaRunnerToken&gt; \\ -e GITEA_RUNNER_NAME=gitea-runner-1 \\ -e GITEA_RUNNER_LABELS=ubuntu-latest:docker://node:16-bullseye,ubuntu-22.04:docker://node:16-bullseye,ubuntu-20.04:docker://node:16-bullseye,ubuntu-18.04:docker://node:16-buster,cth-ubuntu-latest:docker://catthehacker/&gt; --name nazgul-runner-1 \\ -d gitea/act_runner:latest \\ --privileged           Now you should have your first runner up and running, check in Gitea that it’s actually popped up.   Enable actions for all your git repositories that you want, it’s a checkbox named actions under git repository basic settings.   Now lets create a CI/CD pipeline for your git repository. I assume you are capable of creating the Dockerfile needed yourself. The example bellow I took from a project with node backend using the express framework.  # # .gitea/gitea-ci.yaml #  name: Build And Test run-name: $ is runs ci pipeline on: [ push ]  jobs:   build:     runs-on: ubuntu-latest     steps:       - uses: https://github.com/actions/checkout@v4       - name: Use Node.js         uses: https://github.com/actions/setup-node@v3         with:           node-version: '18.17'       - run: npm ci       - run: npm run lint       - run: npm run test       - run: npm run build:prod         env:            NODE_OPTIONS: --max_old_space_size=4096    publish:     runs-on: cth-ubuntu-latest     needs: build     if: gitea.ref == 'refs/heads/main'     steps:       - uses: https://github.com/actions/checkout@v4       - name: Set up Docker Buildx         uses: https://github.com/docker/setup-buildx-action@v3         with:           config-inline: |             [registry.\"&lt;my-private-unsecure-git-repository-ip-address&gt;:5000\"]               http = true               insecure = true                   - name: Build and push Docker image         uses: https://github.com/docker/build-push-action@v5         with:           context: .           file: ./Dockerfile           push: true           tags: \"&lt;my-private-unsecure-git-repository-ip-address&gt;:5000/&lt;my-docker-image&gt;:$,&lt;my-private-unsecure-git-repository-ip-address&gt;:5000/&lt;my-docker-image&gt;:latest\"   So the build pipeline contains two steps, build and publish. The first step executes for all pushes to all branches,  the second step only for main branch. Pay attention to the second step runs on cth-ubuntu-latest.  Which is one of the custom LABELS we added for our runner.   And here is a basic Dockerfile as reference.  # # Dockerfile #  FROM node:alpine WORKDIR /usr/app RUN apk update &amp;&amp; apk add libstdc++ &amp;&amp; apk add build-base &amp;&amp; apk add python3 &amp;&amp; apk add bash &amp;&amp; apk add git COPY package.json . COPY package-lock.json . COPY src/app/public public RUN npm ci COPY . . RUN npm run clean RUN npm run build  CMD [\"npm\", \"run\", \"prod\"]  ","categories": ["ci/cd","docker"],
        "tags": ["docker","docker-registry","gitea","ci/cd"],
        "url": "/ci/cd/docker/2023/10/27/gitea-runner-build-docker-containers.html",
        "teaser": null
      }]
